# offer

## 漫谈

1. 说项目，说平常怎么学习java，说除了项目有自己开发过什么。

   现在正在写一个基于Spring Cloud微服务架构的后台管理系统，实现简单的CURD操作，最重要的是感受微服务架构。主要参考的是github上的另外一个微服务项目。

2. 什么是微服务？

   微服务是基于分布式系统构建的，但是它对于服务的要求更独立更原子，不像SOA更多注重的是代码和功能复用。

## JVM

### 概念

#### 内存泄漏 和 内存溢出

- **内存泄露**：指程序中动态分配内存给一些临时对象，但是对象不会被GC所回收，始终占用内存。即**被分配的对象可达但已无用**。
- **内存溢出**：指程序运行过程中**无法申请到足够的内存**而导致JVM的OOM。

### 实现

#### try catch finally

finally会在return执行之后，返回之前执行。

JVM对他们的实现是，复制每个finally块到每个控制流的最后，保证会执行到。

try catch由方法的异常表执行字节码跳转。

### 运行时内存模型 和 JVM ERROR

需要强调的是**模型与实现的区别**。本节内容只是jvm specification中规定的JAVA虚拟机实现应该遵循的内存模型，如果**需要交代具体实现，必须会特殊说明**。

![概念模型](images\offer\概念模型.png)

#### 线程私有内存

##### 程序计数器PC

程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。**字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。**

**为了线程切换后能恢复到正确的执行位置**，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。

程序计数器是唯一一个不会出现OutOfMemoryError的内存区域，它的生命周期和线程相同。

##### 虚拟机栈

栈帧：和方法调用是一对一的关系。

- 局部变量表：编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型）。

- 操作数栈：

  > Each frame (§2.6) contains a last-in-first-out (LIFO) stack known as its operand stack. The maximum depth of the operand stack of a frame is determined at compile-time and is supplied along with the code for the method associated with the frame (§4.7.3).

**局部变量表和操作数栈的交互即是本栈帧对应方法的字节码的执行。**

**StackOverFlowError：** 若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。

**OutOfMemoryError：** 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。

##### 本地方法栈

同虚拟机栈一样会抛出OOM & SOF。

#### 线程共享内存

##### 堆

存放对象实例。超过最大值会OOM。

##### 方法区

存放对象类型相关的信息。

> It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors, including the special methods used in class and interface initialization <clinit> and in instance initialization <init>.

超过方法区内存限制将抛出OOM。

**hotspot VM JDK8之后对方法区的实现：Metaspace**

Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。

1. 永久代为什么被移出HotSpot JVM了？
   在 HotSpot JVM 中，永久代中用于**存放类和方法的元数据以及常量池**，比如Class和Method。每当一个类初次被加载的时候，它的元数据都会放到永久代中。永久代是有大小限制的，如果加载的类太多，经常导致永久代内存溢出 java.lang.OutOfMemoryError: PermGen ， JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM。

2. Metaspace物理位置
   JDK 8 开始把类的元数据放到本地堆内存(native heap)中，这一块区域就叫 Metaspace，中文名叫元空间。

3. 优点
   OOM问题将不复存在，因为默认的类的元数据分配只受物理内存大小的限制。

   让 Metaspace 变得无限大显然是不现实的，因此我们也要限制 Metaspace 的大小：

   `-XX:MaxMetaspaceSize`

   > By default, the size isn’t limited. The amount of metadata for an application **depends on the application itself, other running applications, and the amount of memory available on the system**.

4. GC
   如果Metaspace的空间占用达到了`-XX:MaxMetaspaceSize`设定的最大值，那么就会触发GC来收集死亡对象和类的加载器。根据JDK 8的特性，G1和CMS都会很好地收集Metaspace区（一般都伴随着Full GC）。

   为了减少垃圾回收的频率及时间，控制吞吐量，对Metaspace进行适当的监控和调优是非常有必要的。如果在Metaspace区发生了频繁的Full GC，那么可能表示存在内存泄露或Metaspace区的空间太小了。

##### 直接内存

NIO（基于Buffer和Channel的非阻塞IO方式）使用Native函数直接分配对外内存。

可能导致各个内存区之和超过物理内存限制从而OOM。

### 对象的内存布局

![分代收集](images\offer\对象内存.png)

其中对象头32位还是64位与虚拟机是多少位有关。

![img](https://img-blog.csdn.net/20140619204318875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGxuanVscA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

#### Mark Word

Mark Word是一个复用的数据结构。在不同的对象状态下，其存储的数据也不尽相同。

![img](https://img-blog.csdn.net/20140619210443906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGxuanVscA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### 垃圾收集

#### 可达性分析算法

通过一系列成为GC roots的对象作为起始点，从这些节点开始向下搜索，搜索过的路径成为引用链，当一个对象没有任何引用链相连，证明这个对象不可用。

#### 垃圾收集算法

| 算法 | 标记清除                                                     | 复制算法                                                     | 标记整理                                             |
| ---- | :----------------------------------------------------------- | :----------------------------------------------------------- | ---------------------------------------------------- |
| 描述 | 将不用的对象标记，然后清除其所在空间。                       | 将内存按容量划分为大小相等的两块，每次使用一块。内存区满就把存活的对象复制到另外一块。 | 每次GC让存活对象向一端移动，然后清理掉边界外的内存。 |
| 优点 | 简单                                                         | 没有碎片问题，实现简单                                       |                                                      |
| 缺点 | 效率低，产生大量的内存碎片，内存碎片多了就无法存储大对象，不得不提前GC | 内存代价高                                                   |                                                      |

#### 分代回收算法

1. 对象优先在Eden中分配

2. 大对象直接进入老年代

3. 长期存活对象直接进入老年代

   对象动态年龄判定，在经历15次MinorGC后进入老年代

![分代收集](images\offer\分代收集.png)

#### 垃圾收集器

| 收集器                | 串行、并行or并发 | 新生代/老年代 | 算法               | 目标         | 适用场景                                  |
| --------------------- | ---------------- | ------------- | ------------------ | ------------ | ----------------------------------------- |
| **Serial**            | 串行             | 新生代        | 复制算法           | 响应速度优先 | 单CPU环境下的Client模式                   |
| **Serial Old**        | 串行             | 老年代        | 标记-整理          | 响应速度优先 | 单CPU环境下的Client模式、CMS的后备预案    |
| **ParNew**            | 并行             | 新生代        | 复制算法           | 响应速度优先 | 多CPU环境时在Server模式下与CMS配合        |
| **Parallel Scavenge** | 并行             | 新生代        | 复制算法           | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |
| **Parallel Old**      | 并行             | 老年代        | 标记-整理          | 吞吐量优先   | 在后台运算而不需要太多交互的任务          |
| **CMS**               | 并发             | 老年代        | 标记-清除          | 响应速度优先 | 集中在互联网站或B/S系统服务端上的Java应用 |
| **G1**                | 并发             | both          | 标记-整理+复制算法 | 响应速度优先 | 面向服务端应用，将来替换CMS               |

##### CMS收集器

**CMS（Concurrent Mark Sweep）**收集器是一种以**获取最短回收停顿时间**为目标的收集器，它非常符合那些集中在互联网站或者B/S系统的服务端上的Java应用，这些应用都非常重视服务的响应速度。从名字上（“Mark Sweep”）就可以看出它是基于**“标记-清除”**算法实现的。

CMS收集器工作的整个流程分为以下4个步骤：

- **初始标记（CMS initial mark）**：仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。
- **并发标记（CMS concurrent mark）**：进行**GC Roots Tracing**的过程，在整个过程中耗时最长。
- **重新标记（CMS remark）**：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。此阶段也需要“Stop The World”。
- **并发清除（CMS concurrent sweep）**

由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过下图可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的时间：

![img](images\offer\f60599b2.png)

**优点**

**并发收集**、**低停顿**：标记和清理都是并发的，STW的初始标记和重新标记时间很短。

**缺点**

- **并发GC，因而对CPU资源非常敏感** 
- **标记-清除算法导致的空间碎片**

##### G1垃圾收集器

G1垃圾收集算法主要应用在多CPU大内存的服务中，在**满足高吞吐量的同时，尽可能的满足垃圾回收时的暂停时间**，该设计主要针对如下应用场景：

- 垃圾收集线程和应用线程并发执行，和CMS一样
- 不希望牺牲太多的吞吐性能
- 可预测的GC暂停时间，在一个长度M ms的时间片中，消耗在GC上的时间不得超过N ms

###### 堆内存结构

1、以往的垃圾回收算法，如CMS，使用的堆内存结构如下：



![img](images\offer\2184951-f6a73e5ef608cfa8.png)



- 新生代：eden space + 2个survivor
- 老年代：old space
- 持久代：1.8之前的perm space
- 元空间：1.8之后的metaspace

这些space必须是地址连续的空间。

2、**在G1算法中，采用了另外一种完全不同的方式组织堆内存**，堆内存被划分为多个大小相等的内存块（Region），每个Region是逻辑连续的一段内存，结构如下：

![img](images\offer\2184951-715388c6f6799bd9-1552460099982.png)



每个Region被标记了E、S、O和H，说明每个Region在运行时都充当了一种角色，其中H是以往算法中没有的，它代表Humongous，这表示这些Region存储的是巨型对象（humongous object，H-obj），当新建对象大小超过Region大小一半时，直接在新的一个或多个连续Region中分配，并标记为H。

**Region**

堆内存中一个Region的大小可以通过`-XX:G1HeapRegionSize`参数指定，大小区间只能是1M、2M、4M、8M、16M和32M，总之是2的幂次方，如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实际大小，具体实现如下：

![img](images\offer\2184951-c6194652e3232be2-1552460118628.png)

默认把堆内存按照2048份均分，最后得到一个合理的大小。

###### GC模式

G1中提供了三种模式垃圾回收模式，young gc、mixed gc 和 full gc，在不同的条件下被触发。

**young gc**

发生在年轻代的GC算法，一般对象（除了巨型对象）都是在eden region中分配内存，当所有eden region被耗尽无法申请内存时，就会触发一次young gc，这种触发机制和之前的young gc差不多，执行完一次young gc，活跃对象会被拷贝到survivor region或者晋升到old region中，空闲的region会被放入空闲列表中，等待下次被使用。

| 参数                    | 含义                                |
| ----------------------- | ----------------------------------- |
| -XX:MaxGCPauseMillis    | 设置G1收集过程目标时间，默认值200ms |
| -XX:G1NewSizePercent    | 新生代最小值，默认值5%              |
| -XX:G1MaxNewSizePercent | 新生代最大值，默认值60%             |

**mixed gc**

当越来越多的对象晋升到老年代old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即mixed gc，该算法并不是一个old gc，除了回收整个young region，还会回收一部分的old region，这里需要注意：是一部分老年代，而不是全部老年代，可以选择哪些old region进行收集，从而可以对垃圾回收的耗时时间进行控制。

那么mixed gc什么时候被触发？

先回顾一下cms的触发机制，如果添加了以下参数：

```
`-XX:CMSInitiatingOccupancyFraction=``80` `-XX:+UseCMSInitiatingOccupancyOnly`
```

当老年代的使用率达到80%时，就会触发一次cms gc。相对的，mixed gc中也有一个阈值参数 `-XX:InitiatingHeapOccupancyPercent`，当老年代大小占整个堆大小百分比达到该阈值时，会触发一次mixed gc.

mixed gc的执行过程有点类似cms，主要分为以下几个步骤：

1. initial mark: 初始标记过程，整个过程STW，标记了从GC Root可达的对象
2. concurrent marking: 并发标记过程，整个过程gc collector线程与应用线程可以并行执行，标记出GC Root可达对象衍生出去的存活对象，并收集各个Region的存活对象信息
3. remark: 最终标记过程，整个过程STW，标记出那些在并发标记过程中遗漏的，或者内部引用发生变化的对象
4. clean up: 垃圾清除过程，如果发现一个Region中没有存活对象，则把该Region加入到空闲列表中

**full gc**

如果对象内存分配速度过快，mixed gc来不及回收，导致老年代被填满，就会触发一次full gc，G1的full gc算法就是单线程执行的serial old gc，会导致异常长时间的暂停时间，需要进行不断的调优，尽可能的避免full gc.

#### Metaspace

如果Metaspace的空间占用达到了设定的最大值，那么就会触发GC来收集死亡对象和类的加载器。G1和CMS都会很好地收集Metaspace区（一般都伴随着Full GC）。

## 并发

### 关键字

#### 乐观锁和悲观锁

##### 悲观锁

假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**。

- `synchronized`和`ReentrantLock`等独占锁
- 行锁，表锁，读锁，写锁，间隙锁

##### 乐观锁

假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据。**乐观锁适用于多读的应用类型，这样可以提高吞吐量**。

应用：

- CAS算法：Atomic类
- 版本号机制：Mysql的快照读
- tryLock 和 自旋锁

#### synchronized 和 volatile

|      | synchronized                                                 | volatile                                                     |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 原理 | 编译为class文件之后会形成monitorenter和monitorexit字节码指令，锁定传入的对象引用，以其作为锁（this，其他的引用，或this.getClass()） | CPU指令                                                      |
| 作用 | 保证当前线程访问代码的独占，确保被操作状态的原子性和可见性   | 将缓存及时刷新到内存和禁止指令重排序的作用，因此只能确保可见性 |

#### BIO 和 NIO

- BIO是阻塞IO，进行IO的线程会一直阻塞到数据IO完毕才结束，一个线程只能处理一个流的I/O事件。

- NIO是非阻塞IO，原理是轮询机制，Linux下底层实现用的是epoll。
  - poll/select：只能无差别轮询所有流，需要找出能读出数据，或者写入数据的流，对他们进行操作。
  - epoll：轮询得到的流都是可处理的流，没有可处理的流时将会阻塞。

### JUC包

#### ConcurrentHashMap

1. hash计算是依赖于put(key, val)中的key的，毕竟key值决定了这个键值对的存储位置和查找方式。如果传入的key值是null，hash为0，否则调用key.hashCode()并将低16位与高16位异或（保证高位的特征被利用上了）。
2. 

#### hashmap线程不安全的表现

- put get 操作不是线程安全的：是非原子操作。
- 扩容不是线程安全的：可能有多个线程同时扩容。

#### ReentrantLock

与内置锁相比的功能点：显式

- 可中断阻塞
- 可尝试获取锁
- 可实现公平锁
- 可以绑定多个Condition

#### CAS

1. CAS底层在`Hotspot`源码的`Unsafe.cpp`中实现，Linux的X86下主要是通过cmpxchg这个指令在CPU级完成CAS操作的。
2. 简述：若内存位置与预期原值匹配则处理器将该位置更新为新值。否则不做操作。
3. Atomic类全都用到了CAS
4. 优点：非阻塞的同步机制，性能好，可伸缩性好（当增加计算资源CPU内存磁盘存储容量和IO带宽时，程序的处理能力相应增加），活跃性好（不会出现饥饿和死锁）
5. 缺点：
   - **ABA问题**：在CAS操作时，带上版本号，每修改一次，版本号+1，之后比较原值的时候还要比较版本号
   -   **循环时间长开销大**：自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销
   - **只能保证一个共享变量的原子操作**：使用锁或者利用`AtomicReference类`把多个共享变量合并成一个共享变量来操作。

#### AQS

AQS是ReentrantLock的实现。

1. 锁状态。state = 0，未被获取；state > 1，被获取。
2. 线程的阻塞和解除阻塞。AQS 中采用了 LockSupport.park(thread) 来挂起线程，用 unpark 来唤醒线程。
3. 阻塞队列。AQS 用的是一个 FIFO 的队列，就是一个链表，每个 node 都持有后继节点的引用。

首先，第一个线程调用 reentrantLock.lock()，翻到最前面可以发现，tryAcquire(1) 直接就返回 true 了，结束。只是设置了 state=1，连 head 都没有初始化，更谈不上什么阻塞队列了。要是线程 1 调用 unlock() 了，才有线程 2 来，那世界就太太太平了，完全没有交集嘛，那我还要 AQS 干嘛。

如果线程 1 没有调用 unlock() 之前，线程 2 调用了 lock(), 想想会发生什么？

线程 2 会初始化 head（new Node()），同时线程 2 也会插入到阻塞队列并挂起 (注意看这里是一个 for 循环，而且设置 head 和 tail 的部分是不 return 的，只有入队成功才会跳出循环)

首先，是线程 2 初始化 head 节点，此时 head==tail, waitStatus==0

![aqs-1](images\offer\aqs-1.png)

然后线程 2 入队：

![aqs-2](images\offer\aqs-2.png)

同时我们也要看此时节点的 waitStatus，我们知道 head 节点是线程 2 初始化的，此时的 waitStatus 没有设置， java 默认会设置为 0，但是到 shouldParkAfterFailedAcquire 这个方法的时候，线程 2 会把后门节点，也就是 head 的waitStatus设置为 -1。

那线程 2 节点此时的 waitStatus 是多少呢，由于没有设置，所以是 0；

如果线程 3 此时再进来，直接插到线程 2 的后面就可以了，此时线程 3 的 waitStatus 是 0，到 shouldParkAfterFailedAcquire 方法的时候把后门节点线程 2 的 waitStatus 设置为 -1。

![aqs-3](images\offer\aqs-3.png)

这里可以简单说下 waitStatus 中 SIGNAL(-1) 状态的意思，Doug Lea 注释的是：代表后继节点需要被唤醒。也就是说这个 waitStatus 其实代表的不是自己的状态，而是后继节点的状态，我们知道，每个 node 在入队的时候，都会把后门节点的状态改为 SIGNAL，然后阻塞，等待被后门唤醒。这里涉及的是两个问题：有线程取消了排队、唤醒操作。

### 锁的分类和优化

#### 重量级锁

JVM使用操作系统的互斥量mutex实现的锁

#### 自旋锁

JDK1.6引入了自适应的自旋锁。

自旋等待避免了线程切换的开销，但当且仅当自旋时间小于线程切换时间才有效果。

自适应是指按照该锁之前的自旋时间和线程执行时间来判断自旋次数。

#### 锁消除

JVM对一些代码上要求同步，但是被检测到不存在共享状态竞争的锁进行消除。

实现：JIT即时编译器逃逸分析技术，判断当前线程栈上SLOT数据都不会逃逸而被其他线程访问到，就可以认为他们是线程私有的，无需加锁。

#### 锁粗化

一段代码如要重复加上和释放同一个锁，JVM会扩展锁的锁定区域（如果不违背代码语义）来减少锁定开销。

#### 轻量级锁

- 目的：在同一时刻没有线程竞争锁时，减少传统重量级锁（JVM使用操作系统的互斥量mutex实现的锁）的使用。

- 使用依据：“对于绝大部分锁，在整个同步代码块内都不存在竞” 的经验法则。

- 实现：**使用CAS操作消除重量级锁同步使用的互斥量。**

  （1）在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝。

  （2）拷贝对象头中的Mark Word复制到锁记录中。

  （3）拷贝成功后，虚拟机将使用**CAS操作**尝试将对象的Mark Word更新为指向Lock Record的指针。如果更新成功，则执行步骤（3），否则执行步骤（4）。

  （4）CAS成功。该线程就拥有该对象的锁，将对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态。

  （5）CAS失败。检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程重入该对象的锁，可以直接进入同步块继续执行。

  （6）否则，说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，当前线程使用自旋来获取锁，后面等待锁的线程也要进入阻塞状态。

- 优点和缺点：满足经验法则的前提下提升性能；多个线程同时竞争锁时降低性能。

#### 偏向锁

目的：数据在无竞争时，消除代码中的同步需求。

实现：

​	（1）确认为可偏向状态。访问Mark Word中偏向锁的标识是否设置成1 && 锁标志位是否为01。

　　（2）如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。

　　（3）如果线程ID并未指向当前线程，则通过CAS操作尝试写入当前线程ID。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。

　　（4）如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂	起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。

　　（5）执行同步代码。

**偏向锁的释放：**

　　偏向锁的撤销在上述第四步骤中有提到。**偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁**。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。

缺点：如果某个对象的锁会被很多不同线程访问，偏向模式就是多余的。

### 线程池

#### 线程池参数

- `corePoolSize`：核心线程数

  - 核心线程会**一直存活**，即便没有任务需要执行

  - **当线程数小于核心线程数时，即使有线程空闲，线程池也会优先创建新线程处理**
  - 设置allowCoreThreadTimeout=true（默认false）时，核心线程会超时关闭

- `maximumPoolSize` ：最大线程数

  - 当线程数>=corePoolSize，且任务队列已满时，线程池会创建新线程来处理任务

  - 当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常

- `keepAliveTime`：线程空闲时间

  当线程空闲时间达到keepAliveTime时，线程会退出，直到线程数量=corePoolSize

- `rejectedExecutionHandler`：任务拒绝处理器

  两种情况会拒绝处理任务：

  - 队列已满，线程数已经达到maxPoolSize，会拒绝新任务
  - 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务

  线程池会调用rejectedExecutionHandler来处理这个任务。如果没有设置默认是AbortPolicy，会抛出异常。ThreadPoolExecutor类有几个内部实现类来处理这类情况：

  - AbortPolicy 丢弃任务，抛运行时异常

  - CallerRunsPolicy 执行任务
  - DiscardPolicy 忽视，什么都不会发生
  - DiscardOldestPolicy 从队列中踢出最先进入队列（最后一个执行）的任务

#### **原理**

线程池按以下行为执行任务：

1. 当线程数小于核心线程数corePoolSize时，创建线程。
2. 当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。
3. 当线程数大于等于核心线程数，且任务队列已满
   1. 若线程数小于最大线程数，创建线程
   2. 若线程数等于最大线程数，抛出异常，拒绝任务

#### SynchronousQueue

当一个线程往队列中写入一个元素时，写入操作不会立即返回，需要等待另一个线程来将这个元素拿走；同理，当一个读线程做读操作的时候，同样需要一个相匹配的写线程的写操作。

因为SynchronousQueue没有存储功能，因此put和take会阻塞，直到有另一个线程已经准备好参与到交付过程中。仅当有足够多的消费者，并且总是有一个消费者准备好获取交付的工作时，才适合使用同步队列。

### 线程安全机制

| 线程安全机制 | 互斥（阻塞）同步                                             | 非阻塞同步                   | 线程封闭                |
| ------------ | ------------------------------------------------------------ | ---------------------------- | ----------------------- |
| 技术         | synchronized，volatile，Lock，Semaphore，CyclicBarrier，CountDownLatch | CAS，Atomic类                | ThreadLocal，无状态的类 |
| 优点         |                                                              |                              |                         |
| 缺点         | 阻塞或者唤醒一个线程，都需要OS从用户态切换到内核态，切换上下文的开销可能比一般的setter getter开销更大 | 只能是一个状态变量的原子操作 | 无法真正意义的并发      |



### 线程

原理：Java线程1：1映射到操作系统原生线程（内核线程、轻量级进程）之上。因此阻塞或者唤醒一个线程，都需要OS从用户态切换到内核态，切换上下文的开销可能比一般的setter getter开销更大。

### TODO:死锁的原因，如何避免



## 计算机网络

### TCP和UDP区别

|            | TCP            | UDP        |
| ---------- | -------------- | ---------- |
| 是否连接   | 面向连接       | 面向非连接 |
| 传输可靠性 | 可靠的         | 不可靠的   |
| 应用场合   | 传输大量的数据 | 少量数据   |
| 速度       | 慢             | 快         |

###  TCP

#### 从IP讲起

IP 协议定义了一套自己的地址规则，称为 IP 地址。它实现了路由功能，允许某个局域网的 A 主机，向另一个局域网的 B 主机发送消息。

![这里写图片描述](https://img-blog.csdn.net/20170611233912957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

路由器就是基于 IP 协议。局域网与局域网之间要靠路由器连接。

路由的原理很简单。市场上所有的路由器，背后都有很多网口，要接入多根网线。路由器内部有一张**路由表**，规定了 A 段 IP 地址走出口一，B 段地址走出口二，……通过这套”指路牌”，实现了数据包的转发。

![这里写图片描述](https://img-blog.csdn.net/20170611233944095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**本机的路由表注明了不同 IP 目的地的数据包**，要发送到哪一个网口（interface）。

**IP 协议只是一个地址协议，并不保证数据包的完整**。如果路由器丢包（比如缓存满了，新进来的数据包就会丢失），就需要发现丢了哪一个包，以及如何重新发送这个包。这就要依靠 TCP 协议。

简单说，**TCP 协议的作用是，保证数据通信的可靠性，防止丢包，在此基础上尽量提高传输性能**。

#### TCP 数据包的大小

以太网数据包（packet）的大小是固定的，最初是1518字节，后来增加到1522字节。其中， 1500 字节是负载（payload），22字节是头信息（head）。
IP 数据包在以太网数据包的负载里面，它也有自己的头信息，最少需要20字节，所以 IP 数据包的负载最多为1480字节。
![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170611234044643?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

（图片说明：IP 数据包在以太网数据包里面，TCP 数据包在 IP 数据包里面。）
TCP 数据包在 IP 数据包的负载里面。它的头信息最少也需要20字节，因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。由于 IP 和 TCP 协议往往有额外的头信息，所以 TCP 负载实际为1400字节左右。
因此，一条1500字节的信息需要两个 TCP 数据包。HTTP/2 协议的一大改进， 就是压缩 HTTP 协议的头信息，使得一个 HTTP 请求可以放在一个 TCP 数据包里面，而不是分成多个，这样就提高了速度。
![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170611234116960?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

（图片说明：以太网数据包的负载是1500字节，TCP 数据包的负载在1400字节左右。）

#### TCP 数据包的编号（SEQ）

一个包1400字节，那么一次性发送大量数据，就必须分成多个包。比如，一个 10MB 的文件，需要发送7100多个包。
发送的时候，TCP 协议为每个包编号（sequence number，简称 SEQ），以便接收的一方按照顺序还原。万一发生丢包，也可以知道丢失的是哪一个包。
第一个包的编号是一个随机数。为了便于理解，这里就把它称为1号包。假定这个包的负载长度是100字节，那么可以推算出下一个包的编号应该是101。这就是说，每个数据包都可以得到两个编号：自身的编号，以及下一个包的编号。接收方由此知道，应该按照什么顺序将它们还原成原始文件。
![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170611234213971?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

（图片说明：当前包的编号是45943，下一个数据包的编号是46183，由此可知，这个包的负载是240字节。）

#### TCP 数据包的组装

收到 TCP 数据包以后，**组装还原是操作系统完成的**。应用程序不会直接处理 TCP 数据包。
对于应用程序来说，不用关心数据通信的细节。除非线路异常，收到的总是完整的数据。应用程序需要的数据放在 TCP 数据包里面，有自己的格式（比如 HTTP 协议）。
**TCP 并没有提供任何机制，表示原始文件的大小，这由应用层的协议来规定**。比如，HTTP 协议就有一个头信息Content-Length，表示信息体的大小。对于操作系统来说，就是持续地接收 TCP 数据包，将它们按照顺序组装好，一个包都不少。
**操作系统不会去处理 TCP 数据包里面的数据。一旦组装好 TCP 数据包，就把它们转交给应用程序。**TCP 数据包的目的端口参数指定了转交给哪个监听该端口的应用程序。系统根据它将组装好的数据转交给相应的应用程序。
![img](https://img-blog.csdn.net/20170611234318166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTmluZ2RheGluZzE5OTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

（图片说明：上图中，21端口是 FTP 服务器，25端口是 SMTP 服务，80端口是 Web 服务器。）
应用程序收到组装好的原始数据，以浏览器为例，就会根据 HTTP 协议的Content-Length字段正确读出一段段的数据。这也意味着，一次 TCP 通信可以包括多个 HTTP 通信。

#### TCP报文首部组成

![img](https://img-blog.csdn.net/20180717201939345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4OTUwMzE2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

1. 源端口和目的端口，各占2个字节，分别写入源端口和目的端口；
2. 序号，占4个字节，**TCP连接中传送的字节流中的每个字节都按顺序编号**。例如，一段报文的序号字段值是 301 ，而携带的数据共有100字段，显然下一个报文段（如果还有的话）的数据序号应该从401开始；
3. 确认号，占4个字节，**是期望收到对方下一个报文的第一个数据字节的序号**。例如，B收到了A发送过来的报文，其序列号字段是501，而数据长度是200字节，这表明B正确的收到了A发送的到序号700为止的数据。因此，B期望收到A的下一个数据序号是701，于是B在发送给A的确认报文段中把确认号置为701；
4. 数据偏移，占4位，它指出TCP报文的数据距离TCP报文段的起始处有多远；
5. 保留，占6位，保留今后使用，但目前应都位0；
6. 紧急URG，当URG=1，表明紧急指针字段有效。告诉系统此报文段中有紧急数据；
   确认ACK，仅当ACK=1时，确认号字段才有效。TCP规定，在连接建立后所有报文的传输都必须把ACK置1；
   推送PSH，当两个应用进程进行交互式通信时，有时在一端的应用进程希望在键入一个命令后立即就能收到对方的响应，这时候就将PSH=1；
   复位RST，当RST=1，表明TCP连接中出现严重差错，必须释放连接，然后再重新建立连接；
   同步SYN，在连接建立时用来同步序号。当SYN=1，ACK=0，表明是连接请求报文，若同意连接，则响应报文中应该使SYN=1，ACK=1；
   终止FIN，用来释放连接。当FIN=1，表明此报文的发送方的数据已经发送完毕，并且要求释放；
7. 窗口，占2字节，窗口大小的内容实际上是接收端接收数据缓冲区的剩余大小；
8. 检验和，占2字节，校验首部和数据这两部分；
9. 紧急指针，占2字节，指出本报文段中的紧急数据的字节数；

#### TCP连接的建立（三次握手）

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170607205709367?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXpjc3U=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![ä¸æ¬¡æ¡æ](https://img-blog.csdn.net/20170605110405666?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXpjc3U=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)**最开始的时候，客户端和服务器都是处于CLOSED状态。主动打开连接的为客户端，被动打开连接的是服务器。**

1. TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态；
   TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时本地生成一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。TCP规定，**SYN=1的报文段不能携带数据，但需要消耗掉一个序号**。
2. TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。TCP规定，**SYN=1, ACK=1 的报文也不能携带数据，但同样要消耗一个序号**。
3. TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，**ACK=1 的报文段可以携带数据，但是如果不携带数据则不消耗序号。**
4. 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。

**为什么TCP客户端最后还要发送一次确认呢？**

- 一句话，**防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误**。

  如果使用的是两次握手建立连接，假设有这样一种场景，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。

  如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。


#### TCP连接的释放（四次挥手）

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170607205756255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXpjc3U=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![åæ¬¡æ¥æ](https://img-blog.csdn.net/20170606084851272?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXpjc3U=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**最开始的时候，客户端和服务器都是处于ESTABLISHED状态，然后客户端主动关闭，服务器被动关闭。**

1. 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。
2. 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。**TCP服务器通知高层的应用进程，客户端向服务器方向的连接被释放。此时服务器处于半关闭状态，即客户端已经没有数据要发送了，但是服务器可能还要向客户端发送数据，客户端依然要接受**。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。
3. 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，**等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）**。
4. **服务器将最后的数据发送完毕后，就向客户端发送连接释放报文**，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。
5. 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，**客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2 * MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。**
6. **服务器只要收到了客户端发出的确认，立即进入CLOSED状态。**同样，撤销TCB后，就结束了这次的TCP连接。可以看到，**服务器结束TCP连接的时间要比客户端早一些**。

**为什么客户端最后还要等待2MSL？**

- MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。

  第一，**保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失**。站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。

  第二，**防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。**客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。

**为什么建立连接是三次握手，关闭连接确是四次挥手呢？**

- 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。

  而关闭连接时，**服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了**。所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。

**如果已经建立了连接，但是客户端突然出现故障了怎么办？**

- TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。



#### 可靠性

TCP协议保证数据传输可靠性的方式有：

**校验和**

发送方：在发送数据之前计算检验和，并进行校验和的填充。 
接收方：收到数据后，对数据以同样的方式进行计算，求出校验和，与发送方的进行比对。

**序列号与确认应答**

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180524103121705?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdWNoZW54aWE4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

1. 序号，占4个字节，**TCP连接中传送的字节流中的每个字节都按顺序编号**。例如，一段报文的序号字段值是 301 ，而携带的数据共有100字段，显然下一个报文段（如果还有的话）的数据序号应该从401开始；
2. 确认号，占4个字节，**是期望收到对方下一个报文的第一个数据字节的序号**。例如，B收到了A发送过来的报文，其序列号字段是501，而数据长度是200字节，这表明B正确的收到了A发送的到序号700为止的数据。因此，B期望收到A的下一个数据序号是701，于是B在发送给A的确认报文段中把确认号置为701；
3. 确认应答：TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文。这个**ACK报文当中带有对应的确认序列号**，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。

**超时重传**

在进行TCP传输时，由于确认应答与序列号机制，也就是说发送方发送一部分数据后，都会等待接收方发送的ACK报文，并解析ACK报文，判断数据是否传输成功。如果发送方发送完数据后，迟迟没有等到接收方的ACK报文，这该怎么办呢？而没有收到ACK报文的原因可能是什么呢？

首先，发送方没有介绍到响应的ACK报文原因可能有两点：

1. 网络原因全体丢包，接收方未接受到任何数据。
2. 接收方接收到了发送方的数据，但响应的ACK报文由于网络原因丢包。

超时重传机制解决了这个问题：**发送方在发送完数据后等待一个时间，时间到达没有接收到ACK报文，那么对刚才发送的数据进行重新发送。**

1. 第一个原因，接收方将在收到二次重发的数据后，便进行ACK应答。

2. 第二个原因，接收方将在收到二次重发的数据后，发现已经已存在（判断依据是序列号，因此序列号还有去除重复数据的作用），直接丢弃，仍旧发送ACK应答。

那么发送方发送完毕后等待的时间是多少呢？如果这个等待的时间过长，那么会影响TCP传输的整体效率，如果等待时间过短，又会导致频繁的发送重复的包。如何权衡？

由于TCP传输时保证能够在任何环境下都有一个高性能的通信，因此**最大超时时间是动态计算的**。

> 在Linux中（BSD Unix和Windows下也是这样）超时以500ms为一个单位进行控制，每次判定超时重发的超时时间都是500ms的整数倍。重发一次后，仍未响应，那么等待2*500ms的时间后，再次重传。等待4*500ms的时间继续重传。以一个指数的形式增长。累计到一定的重传次数，TCP就认为网络或者对端出现异常，强制关闭连接。

**连接管理**

- 三次握手与四次挥手的过程。

**流量控制**

接收端在接收到数据后，对其进行处理。如果发送端的发送速度太快，导致接收端的结束缓冲区很快的填充满了。此时如果发送端仍旧发送数据，那么接下来发送的数据都会丢包，继而导致丢包的一系列连锁反应，超时重传呀什么的。

**TCP根据接收端对数据的处理能力，决定发送端的发送速度，这就是流量控制机制。**

在TCP协议的报头信息当中，有一个16位字段的窗口大小。在介绍这个窗口大小时我们知道，**窗口大小的内容实际上是接收端接收数据缓冲区的剩余大小。**这个数字越大，证明接收端接收缓冲区的剩余空间越大，网络的吞吐量越大。**接收端会在确认应答发送ACK报文时，将自己的即时窗口大小填入，并跟随ACK报文一起发送过去。**而发送方根据ACK报文里的窗口大小的值的改变进而改变自己的发送速度。如果接收到窗口大小的值为0，那么发送方将停止发送数据。并定期的向接收端发送窗口探测数据段，让接收端把窗口大小告诉发送端。 

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180524111634561?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdWNoZW54aWE4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注：16位的窗口大小最大能表示65535个字节（64K），但是TCP的窗口大小最大并不是64K。在TCP首部中40个字节的选项中还包含了一个窗口扩大因子M，实际的窗口大小就是16为窗口字段的值左移M位。每移一位，扩大两倍。

**拥塞控制**

TCP传输的过程中，发送端开始发送数据的时候，**如果刚开始就发送大量的数据，那么就可能造成一些问题。网络可能在开始的时候就很拥堵，如果给网络中在扔出大量数据，那么这个拥堵就会加剧。**拥堵的加剧就会产生大量的丢包，和大量的超时重传，严重影响传输。

所以TCP引入了慢启动的机制，在开始发送数据时，先发送少量的数据探路。探清当前的网络状态如何，再决定多大的速度进行传输。这时候就引入一个叫做拥塞窗口的概念。发送刚开始定义拥塞窗口为 1，每次收到ACK应答，拥塞窗口加 1。**在发送数据之前，首先将拥塞窗口与接收端反馈的窗口大小比对，取较小的值作为实际发送的窗口。**

**拥塞窗口的增长是指数级别的。慢启动的机制只是说明在开始的时候发送的少，发送的慢，但是增长的速度是非常快的。**为了控制拥塞窗口的增长，不能使拥塞窗口单纯的加倍，设置一个拥塞窗口的阈值，当拥塞窗口大小超过阈值时，不能再按照指数来增长，而是线性的增长。在慢启动开始的时候，慢启动的阈值等于窗口的最大值，一旦造成网络拥塞，发生超时重传时，慢启动的阈值会为原来的一半（这里的原来指的是发生网络拥塞时拥塞窗口的大小），同时拥塞窗口重置为 1。 

1.引言

​       **计算机网络中的带宽、交换结点中的缓存和处理机等，都是网络的资源。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就会变坏。这种情况就叫做拥塞。**

​       拥塞控制就是防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。**拥塞控制是一个全局性的过程，和流量控制不同，流量控制指点对点通信量的控制。**

2.慢开始与拥塞避免

​       发送方维持一个叫做**拥塞窗口cwnd（congestion window）**的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口，另外考虑到接受方的接收能力，发送窗口可能小于拥塞窗口。

​       慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。

​       这里用报文段的个数的拥塞窗口大小举例说明慢开始算法，实时拥塞窗口大小是以字节为单位的。如下图：

![img](http://img.blog.csdn.net/20130801220358468?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​       当然收到单个确认但此确认多个数据报的时候就加相应的数值。所以一次传输轮次之后拥塞窗口就加倍。这就是乘法增长，和后面的拥塞避免算法的加法增长比较。

​       为了防止cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。ssthresh的用法如下：

**当cwnd<ssthresh时，使用慢开始算法。**

**当cwnd>ssthresh时，改用拥塞避免算法。**

**当cwnd=ssthresh时，慢开始与拥塞避免算法任意。**

​       拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。

​       无论是在**慢开始阶段**还是在**拥塞避免阶段**，只要发送方判断网络出现拥塞（其根据就是没有收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理），就把慢开始门限设置为出现拥塞时的发送窗口大小的一半。然后把拥塞窗口设置为1，执行慢开始算法。如下图：

![img](http://img.blog.csdn.net/20130801220438375?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​      **再次提醒这里只是为了讨论方便而将拥塞窗口大小的单位改为数据报的个数，实际上应当是字节。**

## 3.快重传和快恢复

​       快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。如下图：

![img](http://img.blog.csdn.net/20130801220556750?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

快重传配合使用的还有快恢复算法，有以下两个要点:

①当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半。但是接下去并不执行慢开始算法。

②考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。如下图：

![img](http://img.blog.csdn.net/20130801220615250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 4.随机早期检测RED

​       以上的拥塞避免算法并没有和网络层联系起来，实际上网络层的策略对拥塞避免算法影响最大的就是路由器的丢弃策略。在简单的情况下路由器通常按照先进先出的策略处理到来的分组。当路由器的缓存装不下分组的时候就丢弃到来的分组，这叫做尾部丢弃策略。这样就会导致分组丢失，发送方认为网络产生拥塞。更为严重的是网络中存在很多的TCP连接，这些连接中的报文段通常是复用路由路径。若发生路由器的尾部丢弃，可能影响到很多条TCP连接，**结果就是这许多的TCP连接在同一时间进入慢开始状态。这在术语中称为全局同步。全局同步会使得网络的通信量突然下降很多，而在网络恢复正常之后，其通信量又突然增大很多。**

​       为避免发生网路中的全局同步现象，路由器采用随机早期检测(RED:randomearly detection)。该算法要点如下：

​       使路由器的队列维持两个参数，即队列长队最小门限min和最大门限max，每当一个分组到达的时候，RED就计算平均队列长度。然后分情况对待到来的分组：

①平均队列长度小于最小门限——把新到达的分组放入队列排队。

②平均队列长度在最小门限与最大门限之间——则按照某一概率将分组丢弃。

③平均队列长度大于最大门限——丢弃新到达的分组。

![img](http://img.blog.csdn.net/20130801220710500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​      以概率p随机丢弃分组，让拥塞控制只在个别的TCP连接上执行，因而避免全局性的拥塞控制。

​       RED的关键就是选择三个参数最小门限、最大门限、丢弃概率和计算平均队列长度。**平均队列长度采用加权平均的方法计算平均队列长度，这和往返时间（RTT）的计算策略是一样的。**

![img](http://img.blog.csdn.net/20130801220741109?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![img](http://img.blog.csdn.net/20130801220802500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 

为了防止网络的拥塞现象，TCP提出了一系列的拥塞控制机制。最初由V. Jacobson在1988年的论文中提出的TCP的拥塞控制由“慢启动(Slow start)”和“拥塞避免(Congestion avoidance)”组成，后来TCP Reno版本中又针对性的加入了“快速重传(Fast retransmit)”、“快速恢复(Fast Recovery)”算法，再后来在TCP NewReno中又对“快速恢复”算法进行了改进，近些年又出现了选择性应答( selective acknowledgement,SACK)算法，还有其他方面的大大小小的改进，成为网络研究的一个热点。

TCP的拥塞控制主要原理依赖于一个拥塞窗口(cwnd)来控制，在之前我们还讨论过TCP还有一个对端通告的接收窗口(rwnd)用于流量控制。窗口值的大小就代表能够发送出去的但还没有收到ACK的最大数据报文段，显然窗口越大那么数据发送的速度也就越快，但是也有越可能使得网络出现拥塞，如果窗口值为1，那么就简化为一个停等协议，每发送一个数据，都要等到对方的确认才能发送第二个数据包，显然数据传输效率低下。TCP的拥塞控制算法就是要在这两者之间权衡，选取最好的cwnd值，从而使得网络吞吐量最大化且不产生拥塞。

由于需要考虑拥塞控制和流量控制两个方面的内容，因此TCP的真正的发送窗口=min(rwnd, cwnd)。但是rwnd是由对端确定的，网络环境对其没有影响，所以在考虑拥塞的时候我们一般不考虑rwnd的值，我们暂时只讨论如何确定cwnd值的大小。关于cwnd的单位，在TCP中是以字节来做单位的，我们假设TCP每次传输都是按照MSS大小来发送数据的，因此你可以认为cwnd按照数据包个数来做单位也可以理解，所以有时我们说cwnd增加1也就是相当于字节数增加1个MSS大小。

慢启动：最初的TCP在连接建立成功后会向网络中发送大量的数据包，这样很容易导致网络中路由器缓存空间耗尽，从而发生拥塞。因此新建立的连接不能够一开始就大量发送数据包，而只能根据网络情况逐步增加每次发送的数据量，以避免上述现象的发生。具体来说，当新建连接时，cwnd初始化为1个最大报文段(MSS)大小，发送端开始按照拥塞窗口大小发送数据，每当有一个报文段被确认，cwnd就增加1个MSS大小。这样cwnd的值就随着网络往返时间(Round Trip Time,RTT)呈指数级增长，事实上，慢启动的速度一点也不慢，只是它的起点比较低一点而已。我们可以简单计算下：

   开始           --->     cwnd = 1

   经过1个RTT后   --->     cwnd = 2*1 = 2

   经过2个RTT后   --->     cwnd = 2*2= 4

   经过3个RTT后   --->     cwnd = 4*2 = 8

如果带宽为W，那么经过RTT*log2W时间就可以占满带宽。

拥塞避免：从慢启动可以看到，cwnd可以很快的增长上来，从而最大程度利用网络带宽资源，但是cwnd不能一直这样无限增长下去，一定需要某个限制。TCP使用了一个叫慢启动门限(ssthresh)的变量，当cwnd超过该值后，慢启动过程结束，进入拥塞避免阶段。对于大多数TCP实现来说，ssthresh的值是65536(同样以字节计算)。拥塞避免的主要思想是加法增大，也就是cwnd的值不再指数级往上升，开始加法增加。此时当窗口中所有的报文段都被确认时，cwnd的大小加1，cwnd的值就随着RTT开始线性增加，这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。

上面讨论的两个机制都是没有检测到拥塞的情况下的行为，那么当发现拥塞了cwnd又该怎样去调整呢？

**首先来看TCP是如何确定网络进入了拥塞状态的，TCP认为网络拥塞的主要依据是它重传了一个报文段**。上面提到过，TCP对每一个报文段都有一个定时器，称为重传定时器(RTO)，当RTO超时且还没有得到数据确认，那么TCP就会对该报文段进行重传，当发生超时时，那么出现拥塞的可能性就很大，某个报文段可能在网络中某处丢失，并且后续的报文段也没有了消息，在这种情况下，TCP反应比较“强烈”：

1.把ssthresh降低为cwnd值的一半

2.把cwnd重新设置为1

3.重新进入慢启动过程。

从整体上来讲，TCP拥塞控制窗口变化的原则是AIMD原则，即加法增大、乘法减小。可以看出TCP的该原则可以较好地保证流之间的公平性，因为一旦出现丢包，那么立即减半退避，可以给其他新建的流留有足够的空间，从而保证整个的公平性。

其实TCP还有一种情况会进行重传：那就是收到3个相同的ACK。TCP在收到乱序到达包时就会立即发送ACK，TCP利用3个相同的ACK来判定数据包的丢失，此时进行快速重传，**快速重传**做的事情有：

1.把ssthresh设置为cwnd的一半

2.把cwnd再设置为ssthresh的值(具体实现有些为ssthresh+3)

3.重新进入拥塞避免阶段。

后来的“快速恢复”算法是在上述的“快速重传”算法后添加的，当收到3个重复ACK时，TCP最后进入的不是拥塞避免阶段，而是快速恢复阶段。快速重传和快速恢复算法一般同时使用。快速恢复的思想是“数据包守恒”原则，即同一个时刻在网络中的数据包数量是恒定的，只有当“老”数据包离开了网络后，才能向网络中发送一个“新”的数据包，如果发送方收到一个重复的ACK，那么根据TCP的ACK机制就表明有一个数据包离开了网络，于是cwnd加1。如果能够严格按照该原则那么网络中很少会发生拥塞，事实上拥塞控制的目的也就在修正违反该原则的地方。

具体来说快速恢复的主要步骤是：

1.当收到3个重复ACK时，把ssthresh设置为cwnd的一半，把cwnd设置为ssthresh的值加3，然后重传丢失的报文段，加3的原因是因为收到3个重复的ACK，表明有3个“老”的数据包离开了网络。 

2.再收到重复的ACK时，拥塞窗口增加1。

3.当收到新的数据包的ACK时，把cwnd设置为第一步中的ssthresh的值。原因是因为该ACK确认了新的数据，说明从重复ACK时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态。

快速重传算法首次出现在4.3BSD的Tahoe版本，快速恢复首次出现在4.3BSD的Reno版本，也称之为Reno版的TCP拥塞控制算法。

可以看出Reno的快速重传算法是针对一个包的重传情况的，然而在实际中，一个重传超时可能导致许多的数据包的重传，因此当多个数据包从一个数据窗口中丢失时并且触发快速重传和快速恢复算法时，问题就产生了。因此NewReno出现了，它在Reno快速恢复的基础上稍加了修改，可以恢复一个窗口内多个包丢失的情况。具体来讲就是：Reno在收到一个新的数据的ACK时就退出了快速恢复状态了，而NewReno需要收到该窗口内所有数据包的确认后才会退出快速恢复状态，从而更一步提高吞吐量。

SACK就是改变TCP的确认机制，最初的TCP只确认当前已连续收到的数据，SACK则把乱序等信息会全部告诉对方，从而减少数据发送方重传的盲目性。比如说序号1，2，3，5，7的数据收到了，那么普通的ACK只会确认序列号4，而SACK会把当前的5，7已经收到的信息在SACK选项里面告知对端，从而提高性能，当使用SACK的时候，NewReno算法可以不使用，因为SACK本身携带的信息就可以使得发送方有足够的信息来知道需要重传哪些包，而不需要重传哪些包。



### HTTP响应码

- 200 - 请求成功
- 301 - 资源（网页等）被永久转移到其它URL
- 404 - 请求的资源（网页等）不存在
- 500 - 内部服务器错误

| 分类 | 分类描述                                       |
| :--- | :--------------------------------------------- |
| 1**  | 信息，服务器收到请求，需要请求者继续执行操作   |
| 2**  | 成功，操作被成功接收并处理                     |
| 3**  | 重定向，需要进一步的操作以完成请求             |
| 4**  | 客户端错误，请求包含语法错误或无法完成请求     |
| 5**  | 服务器错误，服务器在处理请求的过程中发生了错误 |





### 浏览器输入URL发生了什么

#### 一、DNS域名解析

我们在浏览器输入网址，其实就是要向服务器请求我们想要的页面内容，所有浏览器首先要确认的是域名所对应的服务器在哪里。将域名解析成对应的服务器IP地址这项工作，是由DNS服务器来完成的。

客户端收到你输入的域名地址后，它首先去找本地的hosts文件，检查在该文件中是否有相应的域名、IP对应关系，如果有，则向其IP地址发送请求，如果没有，再去找DNS服务器。一般用户很少去编辑修改hosts文件。
![图片描述](https://segmentfault.com/img/bVYTXR?w=1153&h=480)
![图片描述](https://segmentfault.com/img/bVYTXW?w=805&h=478)

浏览器客户端向本地DNS服务器发送一个含有域名www.cnblogs.com的DNS查询报文。本地DNS服务器把查询报文转发到根DNS服务器，根DNS服务器注意到其com后缀，于是向本地DNS服务器返回comDNS服务器的IP地址。本地DNS服务器再次向comDNS服务器发送查询请求，comDNS服务器注意到其www.cnblogs.com后缀并用负责该域名的权威DNS服务器的IP地址作为回应。最后，本地DNS服务器将含有www.cnblogs.com的IP地址的响应报文发送给客户端。

从客户端到本地服务器属于递归查询，而DNS服务器之间的交互属于迭代查询。

正常情况下，本地DNS服务器的缓存中已有comDNS服务器的地址，因此请求根域名服务器这一步不是必需的。

#### 二、建立TCP链接

『三次握手』

#### 三、发送HTTP请求

与服务器建立了连接后，就可以向服务器发起请求了。这里我们先看下请求报文的结构（如下图）：
![图片描述](https://segmentfault.com/img/bVYTYc?w=629&h=169)

请求报文
在浏览器中查看报文首部（以google浏览器为例）：
![图片描述](https://segmentfault.com/img/bVYTYo?w=366&h=273)

#### 四、服务器处理请求

服务器端收到请求后由http服务器处理请求。web服务器解析用户请求，知道了需要调度哪些资源文件，再通过相应的这些资源文件处理用户请求和参数，并调用数据库信息，最后将结果通过web服务器返回给浏览器客户端。

![图片描述](https://segmentfault.com/img/bVYQcl?w=808&h=237)

#### 五、返回响应结果

在HTTP里，有请求就会有响应，哪怕是错误信息。这里我们同样看下响应报文的组成结构：
![图片描述](https://segmentfault.com/img/bVYTYF?w=625&h=168)

响应报文

在响应结果中都会有个一个HTTP状态码，比如我们熟知的200、301、404、500等。通过这个状态码我们可以知道服务器端的处理是否正常，并能了解具体的错误。

状态码由3位数字和原因短语组成。根据首位数字，状态码可以分为五类：
![图片描述](https://segmentfault.com/img/bVYQcs?w=417&h=192)

状态码类别

#### 六、关闭TCP连接

#### 七、浏览器解析HTML

#### 八、浏览器渲染

最后浏览器绘制各个节点，将页面展示给用户。



## 算法

### TOPK

有个大文件，记录形式为：用户昵称 背单词的次数。查找里面10个背单词数量最的人。如果文件1t，内存只有2g怎么做？

#### 堆排序

循环加入多个堆，取出找出每个堆中的TOPK，保存。最后将N个堆中的K个加入新的堆，取出K个。

#### MAPREDUCE

TOPK问题下REDUCE中取出的数据条目数必须是K，不然无法汇总结果。极端情况下可能所有的最终结果都来自一个REDUCE。

### MAPREDUCE

**MapReduce简介**

1. MapReduce是一种分布式计算模型，是Google提出的，主要用于搜索领域，解决海量数据的计算问题。
2. MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算。

 **MapReduce原理**

![img](https://images2015.cnblogs.com/blog/1110462/201703/1110462-20170330105359920-995323028.png)

 

 **MapReduce的执行步骤：**

0、split（**Hadoop接管**）

- 在进行map计算之前，map reduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务，**输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组。**
- 输入分片（input split）往往和hdfs的block（块）关系很密切，假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），换句话说我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。

1、Map用户函数

　　1.1 每一行解析成一个<k, v>。

 		**<0,hello you>   <10,hello me>**

　　1.2 对上一步产生的<k, v>进行处理，转换为新的<k,v>输出。

​		**<hello,1> <you,1>  <hello,1> <me,1>**

　　1.3（**Hadoop接管**） 对上一步输出的<k,v>进行分区，Map ~ shuffle= M：N传送给shuffle，分区过程中其实也包含排序。

2、shuffle（**Hadoop接管**）

 对上一步输出的<k,v>进行分区，Map ~ shuffle= M：N传送给shuffle，分区过程中其实也包含排序。

​	2.1 排序：对不同分区中的数据按照k进行排序。

​		排序后：**<hello,1> <hello,1> <me,1> <you,1>** 

​	2.2 分组（group）：把相同key的所有value放到一个集合中。

​		分组后：**<hello,{1,1}><me,{1}><you,{1}>**

​	2.3 shuffle ~ Reduce = 1：1传送。

- **在Hadoop MapReduce中，框架会确保reduce收到的输入数据是根据key排序、分组过的。**

3、Reduce用户函数

　　2.1 接收的是shuffle排序，分组后的数据，实现自己的业务逻辑。

​		**<hello,{1,1}><me,{1}><you,{1}>** 处理后：**<hello,2> <me,1> <you,1>**

　　2.3 对reduce输出的<k,v>写到HDFS中。



### Morris Traversal

通常，实现二叉树的前序（preorder）、中序（inorder）、后序（postorder）遍历有两个常用的方法：一是递归(recursive)，二是使用栈实现的迭代版本(stack+iterative)。这两种方法都是O(n)的空间复杂度（递归本身占用stack空间或者用户自定义的stack）。

Morris Traversal与前两种方法的不同在于该方法只需要O(1)空间，而且同样可以在O(n)时间内完成。

思考：

- 如果需要O(1)的空间，必须是只使用一个或者少数几个节点引用来遍历二叉树。
- 和使用栈结构相比，最困难的一点在于如何返回上层节点。
- 拿先序遍历（左根右）举例，**如果能够设计出一个“后门”，使得每次在遍历完当前结点的左子树之后，直接跳转回当前结点，就解决了上面的问题。**
- 因为三种遍历方式不变的是左右的顺序，因此可以在左子树遍历的最后一个节点开后门，很容易就能想到应该使这个节点的子节点为当前结点，就可以实现。

#### 一、中序遍历

![这里写图片描述](https://img-blog.csdn.net/20150829152118062)



#### 二、前序遍历
![这里写图片描述](https://img-blog.csdn.net/20150829162244313)



#### 三、后序遍历

**后续遍历稍显复杂，需要建立一个临时节点dump，令其左孩子是root。**并且还需要一个子过程，就是倒序输出某两个节点之间路径上的各个节点。

![这里写图片描述](https://img-blog.csdn.net/20150829162501443)



## 设计模式



## 数据库

### 数据库引擎及比较

| Feature                 | InnoDB | MyISAM |
| ----------------------- | ------ | ------ |
| **Clustered indexes**   | Yes    | No     |
| **Data caches**         | Yes    | No     |
| **Foreign key support** | Yes    | No     |
| **Locking granularity** | Row    | Table  |
| **MVCC**                | Yes    | No     |
| **Transactions**        | Yes    | No     |
| **Storage limits**      | 64TB   | 256TB  |

### 索引原理和实现

操作系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来。

InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB默认每个页的大小为16KB：

```
mysql> show variables like 'innodb_page_size';
```

B-Tree：每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data较大时将会导致每个节点（即一个页）能存储的key的数量小，B-Tree的深度大，增大查询时的磁盘I/O次数。

![20160202204827368](images\offer\20160202204827368)

在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。
![1552547005558](images\offer\20160202205105560)

B+Tree的高度一般都在2~4层。InnoDB存储引擎**根节点常驻内存**，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。

数据库中的B+Tree索引可以分为聚集索引（clustered index）和辅助索引（secondary index）。

- 聚集索引的B+Tree中的非叶子结点存放的是主键上的索引，叶子节点存放的是整张表的行记录数据。
- 辅助索引的叶子节点存储相应行数据的聚集索引键，即主键。
- 辅助索引来查询数据：先遍历辅助索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据。

### 事务

#### 什么是数据库事务

CURD复合原子操作。

#### 数据库事务的性质ACID

原子性(atomicity)
一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚。
一致性(consistency)
数据库总是从一个一致性的状态转换到另外一个一致性的状态，满足业务的一致性约束。
隔离性(isolation)
通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。
持久性(durability)
一且事务提交，则其所傲的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。

#### 隔离级别

![1552548959252](images\offer\捕获.JPG)

- 脏读：事务可以读取其他事务尚未提交的数据。破坏了AI
- 不可重复读：在一个事务内两次SELECT查询相同的数据结果不同。
- 幻读：一个事务插入了新行，当另一个事务的DML操作涉及到该范围的数据行时，会失败，因为幻行被加锁。

#### MVCC：Snapshot Read vs Current Read

- **MVCC的快照读利用版本号解决了不可重复读的问题，但并没有解决幻读。**
- **MVCC的当前读解决了幻读问题，解决方法是GAP锁。**

MySQL InnoDB存储引擎，实现的是基于多版本的并发控制协议MVCC (Multi-Version Concurrency Control) (注：与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control)。MVCC最大的好处：读不加锁，读写不冲突。在读多写少的OLTP (online transaction processing)应用中，极大的增加了系统的并发性能。

在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。

**快照读：**读取的是记录的可见版本 (历史版本)，不用加锁。简单的select操作，属于快照读，不加锁。

- select * from table where ?;

**当前读：**读取的是记录的最新版本，当前读返回的记录都会加上锁，保证其他事务不会再并发修改这条记录。特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。

- select * from table where ? lock in share mode;
- select * from table where ? for update;
- insert into table values (…);
- update table set ? where ?;
- delete from table where ?;

所有以上的语句，都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。

![update](images/offer/medish.jpg)

从图中，可以看到，一个Update操作的具体流程。当Update SQL被发给MySQL后，MySQL Server会根据where条件，读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁 (current read)。待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了一个当前读。同理，Delete操作也一样。Insert操作会稍微有些不同，简单来说，就是Insert操作可能会触发Unique Key的冲突检查，也会进行一个当前读。

**注**：根据上图的交互，针对一条当前读的SQL语句，InnoDB与MySQL Server的交互，是一条一条进行的，因此，加锁也是一条一条进行的。先对一条满足条件的记录加锁，返回给MySQL Server，做一些DML操作；然后在读取下一条加锁，直至读取完毕。

#### 数据库如何实现串行化

```MYSQL
mysql> set global transaction isolation level read committed; //全局的

mysql> set session transaction isolation level read committed; //当前会话
```

### 锁

#### **事务中的加锁是每执行一条语句就加一层锁，直到事务结束释放所有锁。**

#### 判断事务中DML操作的加锁：

- **前提一：**id列是不是主键？

- **前提二：**当前系统的隔离级别是什么？

- **前提三：**id列如果不是主键，那么id列上有索引吗？

- **前提四：**id列上如果有二级索引，那么这个索引是唯一索引吗？

- **前提五：**SQL的执行计划是什么？索引扫描？全表扫描？

`delete from t1 where id = 10;  `  在不同组合下的加锁：

- **组合五：**id列是主键，RR隔离级别

![idä¸"é®+rc](http://pic.yupoo.com/hedengcheng/DnJ6RtaP/medish.jpg)

- **组合六：**id列是二级唯一索引，RR隔离级别

![id unique+rc](http://pic.yupoo.com/hedengcheng/DnJ6PDep/medish.jpg)

- **组合七：**id列是二级非唯一索引，RR隔离级别

![id 非唯一索引 + rr](images\offer\medish1.jpg)

为了保证两次当前读返回一致的记录，那就需要在第一次当前读与第二次当前读之间，其他的事务不会插入新的满足条件的记录并提交。GAP锁不会出现幻读的关键。GAP锁锁住的位置，也不是记录本身，而是两条记录之间的GAP。所谓幻读，就是同一个事务，连续做两次当前读 (例如：select * from t1 where id = 10 for update;)，那么这两次当前读返回的是完全相同的记录 (记录数量一致，记录本身也一致)，第二次的当前读，不会比第一次返回更多的记录 。

- **组合八：**id列上没有索引，RR隔离级别

  ![id æ ç´¢å¼+rr](http://pic.yupoo.com/hedengcheng/DnJ6Rf3q/medish.jpg)

- **组合九：**Serializable隔离级别

  Serializable隔离级别，影响的是`select * from t1 where id = 10;` 这条SQL，在RC，RR隔离级别下，都是快照读，不加锁。但是在Serializable隔离级别，SQL1会加读锁，也就是说快照读不复存在，MVCC并发控制降级为Lock-Based CC。

- **一条复杂的SQL**

  ![å¤æSQL](http://pic.yupoo.com/hedengcheng/DnJ6S3ta/medish.jpg)

  - **Index key：**pubtime > 1 and puptime < 20。此条件，用于确定SQL在idx_t1_pu索引上的查询范围。
  - **Index Filter：**userid = ‘hdc’ 。此条件，可以在idx_t1_pu索引上进行过滤，但不属于Index Key。

  - **Table Filter：**comment is not NULL。此条件，在idx_t1_pu索引上无法过滤，只能在聚簇索引上过滤。

  ![SQLå é](http://pic.yupoo.com/hedengcheng/DnJ6S1s7/medish.jpg)

  在Repeatable Read隔离级别下，针对一个复杂的SQL，首先需要提取其where条件。Index Key确定的范围，需要加上GAP锁；Index Filter过滤条件，在5.6后支持了Index Condition Pushdown，则在index上过滤，不满足Index Filter的记录不用加X锁(图中，用红色箭头标出的X锁，是否要加，视是否支持ICP而定)；Table Filter过滤条件，无论是否满足，都需要加X锁。

### 缓存

#### 页面置换算法Least Recently Used

内存不够的场景下，淘汰掉最不经常使用的数据。

##### LRU原理

假设内存只能容纳3个页大小，按照 7 0 1 2 0 3 0 4 的次序访问页。**假设内存按照栈的方式来描述访问时间，在上面的，是最近访问的，在下面的是，最远时间访问的，LRU就是这样工作的。**

![img](https://pic1.zhimg.com/80/v2-584ed398c35ba76250cfb2f01b20ec0c_hd.jpg)

但是如果让我们自己设计一个基于 LRU 的缓存，这样设计可能问题很多，这段内存按照访问时间进行了排序，会有大量的内存拷贝操作，所以性能肯定是不能接受的。

那么如何设计一个LRU缓存，使得放入和移除都是 O(1) 的，我们需要把访问次序维护起来，但是不能通过内存中的真实排序来反应，有一种方案就是使用双向链表。

##### 基于 HashMap 和 双向链表实现 LRU

整体的设计思路是，可以使用 HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点，如图所示。

![img](https://pic4.zhimg.com/80/v2-09f037608b1b2de70b52d1312ef3b307_hd.jpg)

LRU 存储是基于双向链表实现的，下面的图演示了它的原理。其中 head 代表双向链表的表头，tail 代表尾部。首先预先设置 LRU 的容量，如果存储满了，可以通过 O(1) 的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过 O(1)的效率把新的节点增加到对头，或者把已经存在的节点移动到队头。

下面展示了，预设大小是 3 的，LRU存储的在存储和访问过程中的变化。为了简化图复杂度，图中没有展示 HashMap部分的变化，仅仅演示了上图 LRU 双向链表的变化。我们对这个LRU缓存的操作序列如下：

save("key1", 7)

save("key2", 0)

save("key3", 1)

save("key4", 2)

get("key2")

save("key5", 3)

get("key2")

save("key6", 4)

相应的 LRU 双向链表部分变化如下：

![img](https://pic3.zhimg.com/80/v2-e9a42fee5cdbf9e4e4d23015112fad4e_hd.jpg)s = sav

##### Redis的LRU实现

如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的，具体分析如下：

为了支持LRU，Redis 2.8.19中使用了一个全局的LRU时钟，`server.lruclock`，定义如下，

```cpp
#define REDIS_LRU_BITS 24
unsigned lruclock:REDIS_LRU_BITS; /* Clock for LRU eviction */
```

默认的LRU时钟的分辨率是1秒，可以通过改变`REDIS_LRU_CLOCK_RESOLUTION`宏的值来改变，Redis会在`serverCron()`中调用`updateLRUClock`定期的更新LRU时钟，更新的频率和hz参数有关，默认为`100ms`一次

Redis支持和LRU相关淘汰策略包括，

- `volatile-lru` 设置了过期时间的key参与近似的lru淘汰策略
- `allkeys-lru` 所有的key均参与近似的lru淘汰策略

Redis会基于`server.maxmemory_samples`配置选取固定数目的key，然后比较它们的lru访问时间，然后淘汰最近最久没有访问的key，maxmemory_samples的值越大，Redis的近似LRU算法就越接近于严格LRU算法，但是相应消耗也变高，对性能有一定影响，样本值默认为5。

## Linux

### 命令行



## 框架和中间件

### ZOOKEEPER

#### 抽象模型

Zookeeper提供一个多层级的节点命名空间（节点称为znode），每个节点都用一个以斜杠（/）分隔的路径表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。例如，/foo/doo这个表示一个znode，它的父节点为/foo，父父节点为/，而/为根节点没有父节点。**与文件系统不同的是，这些节点都可以设置关联的数据**，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，**在内存中维护了这个树状的目录结构**，因此**Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M**。

而为了保证高可用，zookeeper需要以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。**客户端在使用zookeeper时，需要知道集群机器列表，通过与集群中的某一台机器建立TCP连接来使用服务**，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。



### REDIS





### NginX





## 分布式

### 概念

高性能：响应速度快

高可用：有请求就必须有响应

### 分布式事务

#### 分布式系统的一致性

一致性就是数据保持一致，在分布式系统中，可以理解为集群中多个节点中数据的值是一模一样的，也可以理解为不同节点的数据是满足某种一致性约束的。

而一致性又可以分为强一致性与弱一致性。

- 强一致性可以理解为在任意时刻，所有节点中的数据是一致的。**同一时间点**，你在节点A中获取到key1的值与在节点B中获取到key1的值应该都是一致的。
- 弱一致性包含很多种不同的实现，目前分布式系统中广泛实现的是最终一致性。

所谓最终一致性，就是不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的相关（一样的或者有一致性约束的）数据总是在向趋同的方向变化。也可以简单的理解为在一段时间后，节点间的数据会最终达到一致状态。

##### 举例：CDN的最终一致性

> 发布一张网页到 CDN，多个服务器有这张网页的副本。后来发现一个错误，需要更新网页，这时只能每个服务器都更新一遍。
>
> 一般来说，网页的更新不是特别强调一致性。短时期内，一些用户拿到老版本，另一些用户拿到新版本，问题不会特别大。当然，所有人最终都会看到新版本。所以，这个场合就是可用性高于一致性。

#### 分布式锁，如何添加，放在什么位置

##### 为什么需要分布式锁？

分布式系统多进程分布在不同机器上，**原单机部署下针对单进程多线程的并发控制锁策略失效**。需要一种跨JVM的互斥机制来控制共享资源的访问，实现最终一致性。

##### 分布式锁和单机锁的区别

- 单机下的单进程（一个JVM）多线程锁是**多线程的**。分布式锁是**多进程的**。
- 多线程由于可以共享堆内存，因此可以**简单的采取为进程分配的内存作为标记存储位置**。而多进程之间不在同一台物理机上，因此**需要将标记存储在一个所有进程都能看到的地方**。

##### 什么是分布式锁？

**分布式锁利用第三方共享存储的锁来实现锁机制。**

- 当在分布式模型下，数据只有一份（或有限制），此时需要**利用锁的技术控制某一时刻修改数据的进程数**。
- **不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题（网络的延时和不可靠）**。
- **分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存。**

##### 分布式锁的实现？

- 基于数据库实现分布式锁； 
- 基于缓存（Redis等）实现分布式锁；
- 基于Zookeeper实现分布式锁；

##### 基于数据库的实现

利用RDBMS表列的唯一性约束，在调用方法时插入方法名，调用结束后删除方法名，就可以实现简陋的分布式锁。

问题：

1、因为是基于数据库实现的，**数据库的可用性和性能将直接影响分布式锁的可用性及性能**，所以，**数据库需要双机部署、数据同步、主备切换**；

2、**不具备可重入的特性**，因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据，所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁；

3、**没有锁失效机制，因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除**，当服务恢复后一直获取不到锁，所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据；

4、**不具备阻塞锁特性，获取不到锁直接返回失败**，所以需要优化获取逻辑，循环多次去获取。

##### 基于Redis的实现方式

命令介绍：

- `SETNX key value`

时间复杂度： O(1)

只在键 `key` 不存在的情况下， 将键 `key` 的值设置为 `value` 。

若键 `key` 已经存在， 则 `SETNX` 命令不做任何动作。

`SETNX` 是『SET if Not eXists』(如果不存在，则 SET)的简写。

返回值：命令在设置成功时返回 `1` ， 设置失败时返回 `0` 。

代码示例：

```
redis> EXISTS job                # job 不存在
(integer) 0

redis> SETNX job "programmer"    # job 设置成功
(integer) 1

redis> SETNX job "code-farmer"   # 尝试覆盖 job ，失败
(integer) 0

redis> GET job                   # 没有被覆盖
"programmer"
```

- `expire`：expire key timeout：为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁。

- `delete`：delete key：删除key

实现思想：

（1）获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID，通过此在释放锁的时候进行判断。

（2）释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。

代码：

```JAVA
public class RedisDistributedLock {

    private static final String LOCK_SUCCESS = "OK";
    private static final Long RELEASE_SUCCESS = 1L;

    /**
     * 尝试获取分布式锁
     * @param jedis Redis客户端
     * @param lockKey 锁
     * @param requestId 请求标识
     * @param expireTime 超期时间
     * @return 是否获取成功
     */
    public static boolean tryLock(Jedis jedis, String lockKey, String requestId, int expireTime) {

        SetParams params = new SetParams();
        //仅当key不存在时设置key
        params.nx();
        params.ex(expireTime);
        String result = jedis.set(lockKey, requestId, params);

        return LOCK_SUCCESS.equals(result);
    }

    /**
     * 释放分布式锁
     * @param jedis Redis客户端
     * @param lockKey 锁
     * @param requestId 请求标识
     * @return 是否释放成功
     */
    public static boolean unlock(Jedis jedis, String lockKey, String requestId) {
        String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        return RELEASE_SUCCESS.equals(result);
    }
```

##### 基于Zookeeper实现分布式锁

实现基础：

1. 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。
2. 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。
3. 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。

算法流程，假设锁空间的根节点为/lock：

1. 客户端连接zookeeper，并在/lock下创建**临时的**且**有序的**子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。
2. 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中**序号最小**的子节点，如果是则认为获得锁，否则**监听刚好在自己之前一位的子节点删除消息**，获得子节点变更通知后重复此步骤直至获得锁；
3. 执行业务代码；
4. 完成业务流程后，删除对应的子节点释放锁。

##### 几种分布式锁实现的比较

从理解的难易程度角度（从易到难）
数据库 > 缓存 > Zookeeper

从实现的复杂性角度（从难到易）
Zookeeper >= 缓存 > 数据库

从性能角度（从高到低）
缓存 > Zookeeper >= 数据库

从可靠性角度（从高到低）
Zookeeper > 缓存 > 数据库